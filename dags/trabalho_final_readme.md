O que deve ser feito?

Criar um Bucket S3

Criar um Cluster EMR (criar manualmente e enviar steps depois ou criar na DAG)

Criar um cluster Kubernetes

Deployar o Airflow no cluster Kubernetes

Criar um usuário chamado airflow-user com permissão de administrador da conta

Escolher um dataset (livre escolha)

Subir o dataset em um bucket S3

Pensar e implementar construção de indicadores e análises sobre esse dataset (produzir 2 ou mais indicadores no mesmo grão)
Escrever no S3 arquivos parquet com as tabelas de indicadores produzidos

Escrever outro job spark que lê todos os indicadores construídos, junta tudo em uma única tabela

Escrever a tabela final de indicadores no S3

Subir um notebook dentro do cluster EMR

Ler a tabela final de indicadores e dar um .show()

Todo o processamento de dados orquestrado pelo AIRFLOW no K8s

Entregáveis Link do repositório git (Github, Gitlab) com os códigos Print do Bucket S3 criado Print do Cluster EMR criado Print da DAG no Airflow concluída (visão do GRID) Print do Notebook mostrando a tabela final com .show() Para envio só serão aceitos arquivos com extensão 'jpg', 'jpeg' ou 'png'.

DEADLINE - 16 de novembro de 2022
